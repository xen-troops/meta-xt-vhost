From b438fa58b9bd06ab48715122ee8ccbc441c29c1c Mon Sep 17 00:00:00 2001
From: Oleksandr Tyshchenko <oleksandr_tyshchenko@epam.com>
Date: Thu, 25 May 2023 19:14:17 +0300
Subject: [PATCH 1/3] vhost_xen: Implement Xen grant mappings module for vhost

A specific module for accessing descriptors in virtio rings which contain
guest grant based addresses instead of pseudo-physical addresses.
Please see Xen grant DMA-mapping layer at drivers/xen/grant-dma-ops.c
which is the origin of such mapping scheme.
The descriptors are mapped in core vhost's translate_desc(), the target
vhost device is responsible for unmapping them as soon as they are
not used anymore. Wire it to vsock device for now.

Signed-off-by: Oleksandr Tyshchenko <oleksandr_tyshchenko@epam.com>
---
 drivers/vhost/Kconfig  |   6 +
 drivers/vhost/Makefile |   3 +
 drivers/vhost/vhost.c  |  34 +++++
 drivers/vhost/vhost.h  |  27 ++++
 drivers/vhost/vsock.c  |  37 ++++++
 drivers/vhost/xen.c    | 284 +++++++++++++++++++++++++++++++++++++++++
 6 files changed, 391 insertions(+)
 create mode 100644 drivers/vhost/xen.c

diff --git a/drivers/vhost/Kconfig b/drivers/vhost/Kconfig
index 587fbae06182..3c85221120e5 100644
--- a/drivers/vhost/Kconfig
+++ b/drivers/vhost/Kconfig
@@ -5,6 +5,11 @@ config VHOST_IOTLB
 	  Generic IOTLB implementation for vhost and vringh.
 	  This option is selected by any driver which needs to support
 	  an IOMMU in software.
+	  
+config VHOST_XEN
+	tristate
+	help
+	  Support of Xen grant mappings for accessing descriptors in virtio rings.
 
 config VHOST_RING
 	tristate
@@ -16,6 +21,7 @@ config VHOST_RING
 config VHOST
 	tristate
 	select VHOST_IOTLB
+	select VHOST_XEN if XEN
 	help
 	  This option is selected by any driver which needs to access
 	  the core of vhost.
diff --git a/drivers/vhost/Makefile b/drivers/vhost/Makefile
index f3e1897cce85..52c1a8e37f19 100644
--- a/drivers/vhost/Makefile
+++ b/drivers/vhost/Makefile
@@ -17,3 +17,6 @@ obj-$(CONFIG_VHOST)	+= vhost.o
 
 obj-$(CONFIG_VHOST_IOTLB) += vhost_iotlb.o
 vhost_iotlb-y := iotlb.o
+
+obj-$(CONFIG_VHOST_XEN) += vhost_xen.o
+vhost_xen-y := xen.o
diff --git a/drivers/vhost/vhost.c b/drivers/vhost/vhost.c
index 5ccb0705beae..601e8b35ea21 100644
--- a/drivers/vhost/vhost.c
+++ b/drivers/vhost/vhost.c
@@ -501,6 +501,9 @@ void vhost_dev_init(struct vhost_dev *dev,
 		vq->heads = NULL;
 		vq->dev = dev;
 		mutex_init(&vq->mutex);
+#ifdef CONFIG_VHOST_XEN
+		INIT_LIST_HEAD(&vq->desc_maps);
+#endif
 		vhost_vq_reset(dev, vq);
 		if (vq->handle_kick)
 			vhost_poll_init(&vq->poll, vq->handle_kick,
@@ -700,6 +703,9 @@ void vhost_dev_cleanup(struct vhost_dev *dev)
 		if (dev->vqs[i]->call_ctx.ctx)
 			eventfd_ctx_put(dev->vqs[i]->call_ctx.ctx);
 		vhost_vq_reset(dev, dev->vqs[i]);
+#ifdef CONFIG_VHOST_XEN
+		vhost_xen_unmap_desc_all(dev->vqs[i]);
+#endif
 	}
 	vhost_dev_free_iovecs(dev);
 	if (dev->log_ctx)
@@ -1445,6 +1451,14 @@ static long vhost_set_memory(struct vhost_dev *d, struct vhost_memory __user *m)
 	for (region = newmem->regions;
 	     region < newmem->regions + mem.nregions;
 	     region++) {
+
+#ifdef CONFIG_VHOST_XEN
+		if (region->guest_phys_addr & XEN_GRANT_DMA_ADDR_OFF) {
+			pr_err("%s: Skip pseudo memory region for Xen grant mappings\n", __func__);
+			continue;
+		}
+#endif
+
 		if (vhost_iotlb_add_range(newumem,
 					  region->guest_phys_addr,
 					  region->guest_phys_addr +
@@ -2038,6 +2052,15 @@ static int translate_desc(struct vhost_virtqueue *vq, u64 addr, u32 len,
 	u64 s = 0;
 	int ret = 0;
 
+#ifdef CONFIG_VHOST_XEN
+	iov->iov_len = len;
+	iov->iov_base = vhost_xen_map_desc(vq, addr, len, access);
+	if (IS_ERR(iov->iov_base))
+		return PTR_ERR(iov->iov_base);
+	else
+		return 1;
+#endif
+
 	while ((u64)len > s) {
 		u64 size;
 		if (unlikely(ret >= iov_size)) {
@@ -2117,7 +2140,12 @@ static int get_indirect(struct vhost_virtqueue *vq,
 			vq_err(vq, "Translation failure %d in indirect.\n", ret);
 		return ret;
 	}
+
+#ifdef CONFIG_VHOST_XEN
+	iov_iter_kvec(&from, READ, (struct kvec *)vq->indirect, ret, len);
+#else
 	iov_iter_init(&from, READ, vq->indirect, ret, len);
+#endif
 	count = len / sizeof desc;
 	/* Buffers are chained via a 16 bit next field, so
 	 * we can have at most 2^16 of these. */
@@ -2179,6 +2207,12 @@ static int get_indirect(struct vhost_virtqueue *vq,
 			*out_num += ret;
 		}
 	} while ((i = next_desc(vq, &desc)) != -1);
+
+#ifdef CONFIG_VHOST_XEN
+	if (vq->indirect)
+		vhost_xen_unmap_desc(vq, vq->indirect->iov_base, vq->indirect->iov_len);
+#endif
+
 	return 0;
 }
 
diff --git a/drivers/vhost/vhost.h b/drivers/vhost/vhost.h
index b063324c7669..822e7e200a9a 100644
--- a/drivers/vhost/vhost.h
+++ b/drivers/vhost/vhost.h
@@ -132,6 +132,15 @@ struct vhost_virtqueue {
 	bool user_be;
 #endif
 	u32 busyloop_timeout;
+
+#ifdef CONFIG_VHOST_XEN
+	/*
+	 * Contains virtio descriptors mapped by using Xen grant mappings
+	 * in order to handle current request. To be unmapped once the request
+	 * is processed.
+	 */
+	struct list_head desc_maps;
+#endif
 };
 
 struct vhost_msg_node {
@@ -228,6 +237,24 @@ int vhost_init_device_iotlb(struct vhost_dev *d, bool enabled);
 void vhost_iotlb_map_free(struct vhost_iotlb *iotlb,
 			  struct vhost_iotlb_map *map);
 
+#define XEN_GRANT_DMA_ADDR_OFF    (1ULL << 63)
+
+#ifdef CONFIG_VHOST_XEN
+void vhost_xen_unmap_desc(struct vhost_virtqueue *vq, void *ptr, u32 size);
+void vhost_xen_unmap_desc_all(struct vhost_virtqueue *vq);
+void *vhost_xen_map_desc(struct vhost_virtqueue *vq, u64 addr, u32 size,
+		int access);
+#else
+static inline void vhost_xen_unmap_desc(struct vhost_virtqueue *vq,
+		void *ptr, u32 size) { }
+static inline void vhost_xen_unmap_desc_all(struct vhost_virtqueue *vq) { }
+static inline void *vhost_xen_map_desc(struct vhost_virtqueue *vq,
+		u64 addr, u32 size, int access)
+{
+	return ERR_PTR(-ENODEV);
+}
+#endif
+
 #define vq_err(vq, fmt, ...) do {                                  \
 		pr_debug(pr_fmt(fmt), ##__VA_ARGS__);       \
 		if ((vq)->error_ctx)                               \
diff --git a/drivers/vhost/vsock.c b/drivers/vhost/vsock.c
index a483cec31d5c..9885cab70ea5 100644
--- a/drivers/vhost/vsock.c
+++ b/drivers/vhost/vsock.c
@@ -80,6 +80,27 @@ static struct vhost_vsock *vhost_vsock_get(u32 guest_cid)
 	return NULL;
 }
 
+#ifdef CONFIG_VHOST_XEN
+static void vhost_vsock_unmap_desc(struct vhost_virtqueue *vq, int count)
+{
+	int i;
+
+	for (i = 0; i < count; i++) {
+		if (vq->iov[i].iov_base)
+			vhost_xen_unmap_desc(vq, vq->iov[i].iov_base, vq->iov[i].iov_len);
+	}
+
+	/*
+	 * Alternatively we could unmap *all* mapped at this point descriptors
+	 * (including indirect) in one go instead of unmapping one by one.
+	 * But we must be sure that doing that we won't end up unmapping
+	 * descriptors which are still in use. This depends on the place(s)
+	 * from which current function gets called.
+	 */
+	/*vhost_xen_unmap_desc_all(vq);*/
+}
+#endif
+
 static void
 vhost_transport_do_send_pkt(struct vhost_vsock *vsock,
 			    struct vhost_virtqueue *vq)
@@ -154,7 +175,11 @@ vhost_transport_do_send_pkt(struct vhost_vsock *vsock,
 			break;
 		}
 
+#ifdef CONFIG_VHOST_XEN
+		iov_iter_kvec(&iov_iter, READ, (struct kvec *)&vq->iov[out], in, iov_len);
+#else
 		iov_iter_init(&iov_iter, READ, &vq->iov[out], in, iov_len);
+#endif
 		payload_len = pkt->len - pkt->off;
 
 		/* If the packet is greater than the space available in the
@@ -186,6 +211,10 @@ vhost_transport_do_send_pkt(struct vhost_vsock *vsock,
 		 */
 		virtio_transport_deliver_tap_pkt(pkt);
 
+#ifdef CONFIG_VHOST_XEN
+		/* Descriptors must be unmapped as soon as they are not used */
+		vhost_vsock_unmap_desc(vq, out + in);
+#endif
 		vhost_add_used(vq, head, sizeof(pkt->hdr) + payload_len);
 		added = true;
 
@@ -336,7 +365,11 @@ vhost_vsock_alloc_pkt(struct vhost_virtqueue *vq,
 		return NULL;
 
 	len = iov_length(vq->iov, out);
+#ifdef CONFIG_VHOST_XEN
+	iov_iter_kvec(&iov_iter, WRITE, (struct kvec *)vq->iov, out, len);
+#else
 	iov_iter_init(&iov_iter, WRITE, vq->iov, out, len);
+#endif
 
 	nbytes = copy_from_iter(&pkt->hdr, sizeof(pkt->hdr), &iov_iter);
 	if (nbytes != sizeof(pkt->hdr)) {
@@ -493,6 +526,10 @@ static void vhost_vsock_handle_tx_kick(struct vhost_work *work)
 		else
 			virtio_transport_free_pkt(pkt);
 
+#ifdef CONFIG_VHOST_XEN
+		/* Descriptors must be unmapped as soon as they are not used */
+		vhost_vsock_unmap_desc(vq, out + in);
+#endif
 		len += sizeof(pkt->hdr);
 		vhost_add_used(vq, head, len);
 		total_len += len;
diff --git a/drivers/vhost/xen.c b/drivers/vhost/xen.c
new file mode 100644
index 000000000000..0b9f1b57c73c
--- /dev/null
+++ b/drivers/vhost/xen.c
@@ -0,0 +1,284 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * A specific module for accessing descriptors in virtio rings which contain
+ * guest grant based addresses instead of pseudo-physical addresses.
+ * Please see Xen grant DMA-mapping layer at drivers/xen/grant-dma-ops.c
+ * which is the origin of such mapping scheme.
+ *
+ * Copyright (C) 2023 EPAM Systems Inc.
+ */
+
+#include <linux/module.h>
+#include <linux/slab.h>
+#include <linux/vhost.h>
+#include <xen/grant_table.h>
+#include <xen/xen.h>
+
+#include "vhost.h"
+
+/* TODO: Make it possible to get domid */
+static domid_t guest_domid = 2;
+
+struct vhost_xen_grant_map {
+	struct list_head next;
+	int count;
+	int flags;
+	grant_ref_t *grefs;
+	domid_t domid;
+	struct gnttab_map_grant_ref *map_ops;
+	struct gnttab_unmap_grant_ref *unmap_ops;
+	struct page **pages;
+	unsigned long vaddr;
+};
+
+static void vhost_xen_free_map(struct vhost_xen_grant_map *map)
+{
+	if (!map)
+		return;
+
+	if (map->pages)
+		gnttab_free_pages(map->count, map->pages);
+
+	kvfree(map->pages);
+	kvfree(map->grefs);
+	kvfree(map->map_ops);
+	kvfree(map->unmap_ops);
+	kfree(map);
+}
+
+static struct vhost_xen_grant_map *vhost_xen_alloc_map(int count)
+{
+	struct vhost_xen_grant_map *map;
+	int i;
+
+	map = kzalloc(sizeof(*map), GFP_KERNEL);
+	if (!map)
+		return NULL;
+
+	map->grefs = kvcalloc(count, sizeof(map->grefs[0]), GFP_KERNEL);
+	map->map_ops = kvcalloc(count, sizeof(map->map_ops[0]), GFP_KERNEL);
+	map->unmap_ops = kvcalloc(count, sizeof(map->unmap_ops[0]), GFP_KERNEL);
+	map->pages = kvcalloc(count, sizeof(map->pages[0]), GFP_KERNEL);
+	if (!map->grefs || !map->map_ops || !map->unmap_ops || !map->pages)
+		goto err;
+
+	if (gnttab_alloc_pages(count, map->pages))
+		goto err;
+
+	for (i = 0; i < count; i++) {
+		map->map_ops[i].handle = -1;
+		map->unmap_ops[i].handle = -1;
+	}
+
+	map->count = count;
+
+	return map;
+
+err:
+	vhost_xen_free_map(map);
+
+	return NULL;
+}
+
+static int vhost_xen_map_pages(struct vhost_xen_grant_map *map)
+{
+	int i, ret = 0;
+
+	if (map->map_ops[0].handle != -1)
+		return -EINVAL;
+
+	for (i = 0; i < map->count; i++) {
+		unsigned long vaddr = (unsigned long)
+			pfn_to_kaddr(page_to_xen_pfn(map->pages[i]));
+		gnttab_set_map_op(&map->map_ops[i], vaddr, map->flags,
+			map->grefs[i], map->domid);
+		gnttab_set_unmap_op(&map->unmap_ops[i], vaddr, map->flags, -1);
+	}
+
+	ret = gnttab_map_refs(map->map_ops, NULL, map->pages, map->count);
+	for (i = 0; i < map->count; i++) {
+		if (map->map_ops[i].status == GNTST_okay)
+			map->unmap_ops[i].handle = map->map_ops[i].handle;
+		else if (!ret)
+			ret = -EINVAL;
+	}
+
+	return ret;
+}
+
+static int vhost_xen_unmap_pages(struct vhost_xen_grant_map *map)
+{
+	int i, ret;
+
+	if (map->unmap_ops[0].handle == -1)
+		return -EINVAL;
+
+	ret = gnttab_unmap_refs(map->unmap_ops, NULL, map->pages, map->count);
+	if (ret)
+		return ret;
+
+	for (i = 0; i < map->count; i++) {
+		if (map->unmap_ops[i].status != GNTST_okay)
+			ret = -EINVAL;
+		map->unmap_ops[i].handle = -1;
+	}
+
+	return ret;
+}
+
+static void vhost_xen_put_map(struct vhost_xen_grant_map *map)
+{
+	if (!map)
+		return;
+
+	if (map->vaddr) {
+		if (map->count > 1)
+			vunmap((void *)map->vaddr);
+		map->vaddr = 0;
+	}
+
+	if (map->pages) {
+		int ret;
+
+		ret = vhost_xen_unmap_pages(map);
+		if (ret)
+			pr_err("%s: Failed to unmap pages from dom%d (ret=%d)\n",
+					__func__, map->domid, ret);
+	}
+	vhost_xen_free_map(map);
+}
+
+static struct vhost_xen_grant_map *vhost_xen_find_map(struct vhost_virtqueue *vq,
+		unsigned long vaddr, int count)
+{
+	struct vhost_xen_grant_map *map;
+
+	list_for_each_entry(map, &vq->desc_maps, next) {
+		if (map->vaddr != vaddr)
+			continue;
+		if (count && map->count != count)
+			continue;
+		return map;
+	}
+
+	return NULL;
+}
+
+void vhost_xen_unmap_desc_all(struct vhost_virtqueue *vq)
+{
+	struct vhost_xen_grant_map *map;
+
+	if (!xen_domain())
+		return;
+
+	while (!list_empty(&vq->desc_maps)) {
+		map = list_entry(vq->desc_maps.next, struct vhost_xen_grant_map, next);
+		list_del(&map->next);
+
+		pr_debug("%s: dom%d: vaddr 0x%lx count %u\n",
+				__func__, map->domid, map->vaddr, map->count);
+		vhost_xen_put_map(map);
+	}
+}
+
+void *vhost_xen_map_desc(struct vhost_virtqueue *vq, u64 addr, u32 size, int access)
+{
+	struct vhost_xen_grant_map *map;
+	unsigned long offset = xen_offset_in_page(addr);
+	int count = XEN_PFN_UP(offset + size);
+	int i, ret;
+
+	if (!xen_domain() || guest_domid == DOMID_INVALID)
+		return ERR_PTR(-ENODEV);
+
+	if (!(addr & XEN_GRANT_DMA_ADDR_OFF)) {
+		pr_err("%s: Descriptor from dom%d cannot be mapped (0x%llx is not a Xen grant address)\n",
+				__func__, guest_domid, addr);
+		return ERR_PTR(-EINVAL);
+	}
+
+	map = vhost_xen_alloc_map(count);
+	if (!map)
+		return ERR_PTR(-ENOMEM);
+
+	map->domid = guest_domid;
+	for (i = 0; i < count; i++)
+		map->grefs[i] = ((addr & ~XEN_GRANT_DMA_ADDR_OFF) >> XEN_PAGE_SHIFT) + i;
+
+	map->flags |= GNTMAP_host_map;
+	if (access == VHOST_ACCESS_RO)
+		map->flags |= GNTMAP_readonly;
+
+	ret = vhost_xen_map_pages(map);
+	if (ret) {
+		pr_err("%s: Failed to map pages from dom%d (ret=%d)\n",
+				__func__, map->domid, ret);
+		vhost_xen_put_map(map);
+		return ERR_PTR(ret);
+	}
+
+	/*
+	 * Consider allocating xen_alloc_unpopulated_contiguous_pages() instead of
+	 * xen_alloc_unpopulated_pages() to avoid maping as with the later
+	 * map->pages are not guaranteed to be contiguous.
+	 */
+	if (map->count > 1) {
+		map->vaddr = (unsigned long)vmap(map->pages, map->count, VM_MAP,
+				PAGE_KERNEL);
+		if (!map->vaddr) {
+			pr_err("%s: Failed to create virtual mappings\n", __func__);
+			vhost_xen_put_map(map);
+			return ERR_PTR(-ENOMEM);
+		}
+	} else
+		map->vaddr = (unsigned long)pfn_to_kaddr(page_to_xen_pfn(map->pages[0]));
+
+	list_add_tail(&map->next, &vq->desc_maps);
+
+	pr_debug("%s: dom%d: addr 0x%llx size 0x%x (access 0x%x) -> vaddr 0x%lx count %u (paddr 0x%llx)\n",
+			__func__, map->domid, addr, size, access, map->vaddr, count,
+			page_to_phys(map->pages[0]));
+
+	return (void *)(map->vaddr + offset);
+}
+
+void vhost_xen_unmap_desc(struct vhost_virtqueue *vq, void *ptr, u32 size)
+{
+	struct vhost_xen_grant_map *map;
+	unsigned long offset = xen_offset_in_page(ptr);
+	int count = XEN_PFN_UP(offset + size);
+
+	if (!xen_domain())
+		return;
+
+	map = vhost_xen_find_map(vq, (unsigned long)ptr & XEN_PAGE_MASK, count);
+	if (map) {
+		list_del(&map->next);
+
+		pr_debug("%s: dom%d: vaddr 0x%lx count %u\n",
+				__func__, map->domid, map->vaddr, map->count);
+		vhost_xen_put_map(map);
+	}
+}
+
+static int __init vhost_xen_init(void)
+{
+	if (!xen_domain())
+		return -ENODEV;
+
+	pr_info("%s: Initialize module for Xen grant mappings\n", __func__);
+
+	return 0;
+}
+
+static void __exit vhost_xen_exit(void)
+{
+
+}
+
+module_init(vhost_xen_init);
+module_exit(vhost_xen_exit);
+
+MODULE_DESCRIPTION("Xen grant mappings module for vhost");
+MODULE_AUTHOR("Oleksandr Tyshchenko <oleksandr_tyshchenko@epam.com>");
+MODULE_LICENSE("GPL v2");
-- 
2.34.1

