From b6f4784315a551aaf3c20513c7bb987c0f573966 Mon Sep 17 00:00:00 2001
From: Oleksandr Tyshchenko <oleksandr_tyshchenko@epam.com>
Date: Thu, 15 Jun 2023 17:27:33 +0300
Subject: [PATCH] vhost_xen: Adapt net for Xen specific mappings

Please note, the following configs should be enabled in defconfig:
 CONFIG_VHOST_NET=y
 CONFIG_TAP=y
 CONFIG_MACVLAN=y
 CONFIG_MACVTAP=y

Signed-off-by: Oleksandr Tyshchenko <oleksandr_tyshchenko@epam.com>
---
 drivers/vhost/net.c | 42 ++++++++++++++++++++++++++++++++++++++++++
 1 file changed, 42 insertions(+)

diff --git a/drivers/vhost/net.c b/drivers/vhost/net.c
index c8784dfafdd7..6f8542535afe 100644
--- a/drivers/vhost/net.c
+++ b/drivers/vhost/net.c
@@ -148,6 +148,27 @@ struct vhost_net {
 
 static unsigned vhost_net_zcopy_mask __read_mostly;
 
+#ifdef CONFIG_VHOST_XEN
+static void vhost_net_unmap_desc(struct vhost_virtqueue *vq, int count)
+{
+	int i;
+
+	for (i = 0; i < count; i++) {
+		if (vq->iov[i].iov_base)
+			vhost_xen_unmap_desc(vq, vq->iov[i].iov_base, vq->iov[i].iov_len);
+	}
+
+	/*
+	 * Alternatively we could unmap *all* mapped at this point descriptors
+	 * (including indirect) in one go instead of unmapping one by one.
+	 * But we must be sure that doing that we won't end up unmapping
+	 * descriptors which are still in use. This depends on the place(s)
+	 * from which current function gets called.
+	 */
+	/*vhost_xen_unmap_desc_all(vq);*/
+}
+#endif
+
 static void *vhost_net_buf_get_ptr(struct vhost_net_buf *rxq)
 {
 	if (rxq->tail != rxq->head)
@@ -599,7 +620,11 @@ static size_t init_iov_iter(struct vhost_virtqueue *vq, struct iov_iter *iter,
 	/* Skip header. TODO: support TSO. */
 	size_t len = iov_length(vq->iov, out);
 
+#ifdef CONFIG_VHOST_XEN
+	iov_iter_kvec(iter, WRITE, (struct kvec *)vq->iov, out, len);
+#else
 	iov_iter_init(iter, WRITE, vq->iov, out, len);
+#endif
 	iov_iter_advance(iter, hdr_size);
 
 	return iov_iter_count(iter);
@@ -841,6 +866,11 @@ static void handle_tx_copy(struct vhost_net *net, struct socket *sock)
 		vq->heads[nvq->done_idx].id = cpu_to_vhost32(vq, head);
 		vq->heads[nvq->done_idx].len = 0;
 		++nvq->done_idx;
+
+#ifdef CONFIG_VHOST_XEN
+		/* Descriptors must be unmapped as soon as they are not used */
+		vhost_net_unmap_desc(vq, out);
+#endif
 	} while (likely(!vhost_exceeds_weight(vq, ++sent_pkts, total_len)));
 
 	vhost_tx_batch(net, nvq, sock, &msg);
@@ -1173,14 +1203,22 @@ static void handle_rx(struct vhost_net *net)
 			msg.msg_control = vhost_net_buf_consume(&nvq->rxq);
 		/* On overrun, truncate and discard */
 		if (unlikely(headcount > UIO_MAXIOV)) {
+#ifdef CONFIG_VHOST_XEN
+			iov_iter_kvec(&msg.msg_iter, READ, (struct kvec *)vq->iov, 1, 1);
+#else
 			iov_iter_init(&msg.msg_iter, READ, vq->iov, 1, 1);
+#endif
 			err = sock->ops->recvmsg(sock, &msg,
 						 1, MSG_DONTWAIT | MSG_TRUNC);
 			pr_debug("Discarded rx packet: len %zd\n", sock_len);
 			continue;
 		}
 		/* We don't need to be notified again. */
+#ifdef CONFIG_VHOST_XEN
+		iov_iter_kvec(&msg.msg_iter, READ, (struct kvec *)vq->iov, in, vhost_len);
+#else
 		iov_iter_init(&msg.msg_iter, READ, vq->iov, in, vhost_len);
+#endif
 		fixup = msg.msg_iter;
 		if (unlikely((vhost_hlen))) {
 			/* We will supply the header ourselves
@@ -1224,6 +1262,10 @@ static void handle_rx(struct vhost_net *net)
 			goto out;
 		}
 		nvq->done_idx += headcount;
+#ifdef CONFIG_VHOST_XEN
+		/* Descriptors must be unmapped as soon as they are not used */
+		vhost_net_unmap_desc(vq, in);
+#endif
 		if (nvq->done_idx > VHOST_NET_BATCH)
 			vhost_net_signal_used(nvq);
 		if (unlikely(vq_log))
-- 
2.34.1

